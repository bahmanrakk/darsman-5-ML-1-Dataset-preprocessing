import sys
from pathlib import Path

import pandas as pd
import numpy as np

import os
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split  

# ---------------------------------------------------------------------------------------------------
# 1) Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¯Ø± DataFrame
# ---------------------------------------------------------------------------------------------------
def standardize_nan_values(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ØªÙ†ÛŒ Ø±Ø§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯Ù…Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ np.nan ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„ØŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ÛŒ/Ø§Ù†ØªÙ‡Ø§ÛŒ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ Ø±Ø§ Ù‡Ù…Ù‡â€ŒØ¬Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    print("\nğŸ”„ Ø´Ø±ÙˆØ¹ Ø¨Ø®Ø´ 2: Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ùˆ Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§")

    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ÛŒ/Ø§Ù†ØªÙ‡Ø§ÛŒ Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ù…Ù‡â€ŒÛŒ Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§
    df = df.replace(r"^\s+|\s+$", "", regex=True)

    # Ø³Ù¾Ø³ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
    nan_pattern = r'(?i)^(nan|na|n/a|null|none|missing|-|\.)?$'
    df = df.replace(nan_pattern, np.nan, regex=True)

    print("âš™ï¸ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ø­Ø°Ù Ùˆ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù†Ø¯.")
    return df
# ---------------------------------------------------------------------------------------------------
# 2) Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª
# ---------------------------------------------------------------------------------------------------
def load_and_inspect_data(file_path: str) -> pd.DataFrame:
    """
    Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ CSV Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:
      1. Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
      2. Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
      3. Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
      4. Ù†Ù…Ø§ÛŒØ´ Ø´Ú©Ù„ØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø³Ø§Ø®ØªØ§Ø±ØŒ Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒØŒ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ùˆ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
      5. Ù„ÛŒØ³Øª Ú©Ø±Ø¯Ù† Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
    """
    print("\nğŸ”„ Ø´Ø±ÙˆØ¹ Ø¨Ø®Ø´ 1: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª")
    path = Path(file_path)
    # 2.1 Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„
    print("  â†³ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„")
    if not path.is_file():
        print(f"ğŸŸ¥ ÙØ§ÛŒÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯: {file_path}")
        sys.exit(1)

    # 2.2 Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
    print("  â†³ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…")
    df = pd.read_csv(path)
    print(f"âœ… Ø¯ÛŒØªØ§Ø³Øª '{path.name}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")

    # 2.3 Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
    df = standardize_nan_values(df)

    # 2.4 Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
    print("\n  â†³ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§")
    df.columns = (
        df.columns
          .str.strip()
          .str.replace(r"\s+", "_", regex=True)
    )
    print("âš™ï¸ Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ù¾Ø³ Ø§Ø² Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ:", df.columns.tolist(), sep="\n")

    # 2.5 Ù†Ù…Ø§ÛŒØ´ Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
    print(
        "\n ğŸ”¢ Ø´Ú©Ù„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…:",
        f"{df.shape[0]} Ø³Ø·Ø± x {df.shape[1]} Ø³ØªÙˆÙ†",
        sep="\n"
    )

    # 2.6 Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
    print("\n--- A1: Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ---")
    df.info()

    # 2.7 Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
    print("\n--- B1: Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ (Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ) ---")
    print(df.describe())

    # 2.8 Ø¢Ù…Ø§Ø± Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ
    print("\n--- C1: Ø¢Ù…Ø§Ø± Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ ---")
    print(df.describe(include=['object', 'category']).T)

    # 2.9 Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ (5 Ø³Ø·Ø± Ø§ÙˆÙ„)
    print("\n--- D1: Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ (5 Ø³Ø·Ø± Ø§ÙˆÙ„) ---")
    print(df.head(5))

    # 2.10 ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
    print("\n--- E1: ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ ---")
    missing = df.isna().sum()
    if missing.any():
        print(missing[missing > 0])
    else:
        print("Ø³ØªÙˆÙ†ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

    # 2.11 Ù„ÛŒØ³Øª Ù…ØªØºÛŒØ±Ù‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
    print("\n--- F1: Ú¯Ø±ÙˆÙ‡ Ø¨Ù†Ø¯ÛŒ Ø³ØªÙˆÙ† Ù‡Ø§ ---")
    num_features = df.select_dtypes(include=[np.number]).columns.tolist()
    cat_features = df.select_dtypes(include=['object', 'category']).columns.tolist()
    print(
        f"\nÙ…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ ({len(num_features)}):",
        f"{num_features}",
        sep="\n"
    )
    print(
        f"\nÙ…ØªØºÛŒØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¹Ø¯Ø¯ÛŒ ({len(cat_features)}):",
        f"{cat_features}",
        sep="\n"
    )
    return df

# ---------------------------------------------------------------------------------------------------
# 3) ØªØ­Ù„ÛŒÙ„ Ùˆ Ø¨ØµØ±ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª (Outliers)
# ---------------------------------------------------------------------------------------------------
def analyze_outliers(df: pd.DataFrame, columns: list):
    """
    ÙÙ‚Ø· Ø¢Ù…Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    print("\n--- ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª ---")
    # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù…Ø¹ØªØ¨Ø±
    valid_cols = [col for col in columns if col in df.columns]
    numeric_cols = df[valid_cols].select_dtypes(include=np.number).columns.tolist()

    if not numeric_cols:
        print("âŒ Ù‡ÛŒÚ† Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return

    print(f"ğŸ“Š Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„: {numeric_cols}")

    for col in numeric_cols:
        q1 = df[col].quantile(0.25)
        median = df[col].median()
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_fence = q1 - 1.5 * iqr
        upper_fence = q3 + 1.5 * iqr
        outliers_lower = df[df[col] < lower_fence]
        outliers_upper = df[df[col] > upper_fence]
        total_outliers = len(outliers_lower) + len(outliers_upper)
        non_outliers = df[(df[col] >= lower_fence) & (df[col] <= upper_fence)]
        min_val = non_outliers[col].min()
        max_val = non_outliers[col].max()
        mean_val = df[col].mean()
        std_dev = df[col].std()
        skewness = df[col].skew()
        kurtosis = df[col].kurtosis()

        if skewness == 0:
            skew_interpret = "Ù…ØªÙ‚Ø§Ø±Ù† Ø§Ø³Øª"
        elif skewness > 0:
            skew_interpret = "Ú†ÙˆÙ„Ú¯ÛŒ Ø±Ø§Ø³Øª Ø¯Ø§Ø±Ø¯"
        else:
            skew_interpret = "Ú†ÙˆÙ„Ú¯ÛŒ Ú†Ù¾ Ø¯Ø§Ø±Ø¯"

        if kurtosis == 0:
            kurt_interpret = "Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ù†Ø±Ù…Ø§Ù„"
        elif kurtosis > 0:
            kurt_interpret = "Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ù…Ø«Ø¨Øª (Ø¯Ù… Ø³Ù†Ú¯ÛŒÙ†)"
        else:
            kurt_interpret = "Ú©Ø´ÛŒØ¯Ú¯ÛŒ Ù…Ù†ÙÛŒ (Ø¯Ù… Ø³Ø¨Ú©)"

        stats_summary = (
            f"\n--- Ø¢Ù…Ø§Ø± Ø³ØªÙˆÙ† '{col}' ---\n"
            f"  - Q1 (Ú†Ø§Ø±Ú© Ø§ÙˆÙ„): {q1:,.2f}\n"
            f"  - Median (Q2 - Ù…ÛŒØ§Ù†Ù‡): {median:,.2f}\n"
            f"  - Q3 (Ú†Ø§Ø±Ú© Ø³ÙˆÙ…): {q3:,.2f}\n"
            f"  - IQR (Ø¯Ø§Ù…Ù†Ù‡ Ø¨ÛŒÙ† Ú†Ø§Ø±Ú©ÛŒ): {iqr:,.2f}\n"
            f"  - Ù…Ø±Ø² Ù¾Ø§ÛŒÛŒÙ† (Lower Fence): {lower_fence:,.2f}\n"
            f"  - Ù…Ø±Ø² Ø¨Ø§Ù„Ø§ (Upper Fence): {upper_fence:,.2f}\n"
            f"  - Ø­Ø¯Ø§Ù‚Ù„ Ù…Ù‚Ø¯Ø§Ø± (Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø²): {min_val:,.2f}\n"
            f"  - Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ù‚Ø¯Ø§Ø± (Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø¬Ø§Ø²): {max_val:,.2f}\n"
            f"  - ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª: {total_outliers} (Ù¾Ø§ÛŒÛŒÙ†: {len(outliers_lower)}, Ø¨Ø§Ù„Ø§: {len(outliers_upper)})\n"
            f"  - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {mean_val:,.2f}\n"
            f"  - Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±: {std_dev:,.2f}\n"
            f"  - Ú†ÙˆÙ„Ú¯ÛŒ (Skewness): {skewness:,.4f} ({skew_interpret})\n"
            f"  - Ú©Ø´ÛŒØ¯Ú¯ÛŒ (Kurtosis): {kurtosis:,.4f} ({kurt_interpret})"
        )
        print(stats_summary)


def visualize_outliers(df: pd.DataFrame, columns: list, save_base_name: str = "outlier_analysis"):
    """
    ÙÙ‚Ø· Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Box Plot Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø¨Ø¯ÙˆÙ† Ú†Ø§Ù¾ Ø¢Ù…Ø§Ø±).
    """
    print("\n--- Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Box Plot Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª ---")
    output_dir = "data"
    os.makedirs(output_dir, exist_ok=True)
    valid_cols = [col for col in columns if col in df.columns]
    numeric_cols = df[valid_cols].select_dtypes(include=np.number).columns.tolist()

    if not numeric_cols:
        print("âŒ Ù‡ÛŒÚ† Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return

    # Ø¨Ø®Ø´ Ø§ÙˆÙ„: Box Plot Ø§Ø³ØªØ§ØªÛŒÚ© Ø¨Ø§ Matplotlib/Seaborn
    print("\nğŸ“ˆ Ø³Ø§Ø®Øª Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§ØªÛŒÚ© ...")
    plt.style.use('seaborn-v0_8-whitegrid')
    num_plots = len(numeric_cols)
    fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 5), squeeze=False)
    for i, col in enumerate(numeric_cols):
        sns.boxplot(y=df[col], ax=axes[0, i], color="skyblue")
        axes[0, i].set_title(f'Box Plot for "{col}"', fontsize=14)
        axes[0, i].set_ylabel('Ù…Ù‚Ø¯Ø§Ø±', fontsize=12)
        axes[0, i].set_xlabel('')
    plt.tight_layout()
    plt.show()

    # Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Box Plot ØªØ¹Ø§Ù…Ù„ÛŒ Ø¨Ø§ Plotly
    print("\nğŸŒ Ø³Ø§Ø®Øª Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ (HTML) ...")
    from plotly.subplots import make_subplots
    import plotly.graph_objects as go

    fig_combined = make_subplots(rows=1, cols=len(numeric_cols), subplot_titles=numeric_cols)
    for i, col in enumerate(numeric_cols, 1):
        fig_combined.add_trace(go.Box(y=df[col], name=col, boxpoints='outliers', boxmean=True), row=1, col=i)
    fig_combined.update_layout(title_text="ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª - Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ±Ú©ÛŒØ¨ÛŒ", showlegend=False)
    combined_file_path = os.path.join(output_dir, f"{save_base_name}_combined.html")
    fig_combined.write_html(combined_file_path)
    print(f"  - âœ”ï¸ Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ±Ú©ÛŒØ¨ÛŒ Ø¯Ø±: {combined_file_path}")

    for col in numeric_cols:
        fig_single = go.Figure()
        fig_single.add_trace(go.Box(y=df[col], name=col, boxpoints='all', jitter=0.3, pointpos=-1.8))
        fig_single.update_layout(title=f'ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ø¨Ø±Ø§ÛŒ "{col}"')
        single_file_path = os.path.join(output_dir, f"{save_base_name}_{col}.html")
        fig_single.write_html(single_file_path)
        print(f"  - âœ”ï¸ Ù†Ù…ÙˆØ¯Ø§Ø± ØªÚ©ÛŒ Ø³ØªÙˆÙ† '{col}' Ø¯Ø±: {single_file_path}")

# ---------------------------------------------------------------------------------------------------
# 4) ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ Ø¨Ø± Ø±ÙˆÛŒ ÛŒÚ© Ø³ØªÙˆÙ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ
# ---------------------------------------------------------------------------------------------------
def visualize_distribution(df, column, bins='auto', kde=False, max_categories=50):
    """
    Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø³ØªÙˆÙ†:
     - Ø§Ú¯Ø± Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§Ø´Ø¯: Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø¨Ø§ Ú¯Ø²ÛŒÙ†Ù‡ KDE (ØªØ®Ù…ÛŒÙ† Ú†Ú¯Ø§Ù„ÛŒ)
     - Ø§Ú¯Ø± Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø§Ø´Ø¯: Ù†Ù…ÙˆØ¯Ø§Ø± Ø´Ù…Ø§Ø±Ø´ (Bar chart)
    Args:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
        column (str): Ù†Ø§Ù… Ø³ØªÙˆÙ†
        bins (int or 'auto'): ØªØ¹Ø¯Ø§Ø¯ Ø³Ø·Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù…
        kde (bool): Ù†Ù…Ø§ÛŒØ´ KDE Ø¨Ø±Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        max_categories (int): Ø¨ÛŒØ´ÛŒÙ†Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø³ØªÙ‡â€Œ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ (Ø¨Ø§Ø±Ú†Ø§Ø±Øª)
    """
    import matplotlib.pyplot as plt
    import seaborn as sns

    if column not in df.columns:
        print(f"âŒ Ø³ØªÙˆÙ† '{column}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return

    col_data = df[column].dropna()
    plt.figure(figsize=(10, 5))
    if pd.api.types.is_numeric_dtype(col_data):
        sns.histplot(col_data, bins=bins, kde=kde, color="skyblue")
        plt.title(f"Histogram of {column}")
        plt.xlabel(column)
        plt.ylabel("ØªØ¹Ø¯Ø§Ø¯")
    else:
        value_counts = col_data.value_counts()
        if len(value_counts) > max_categories:
            value_counts = value_counts.head(max_categories)
            plt.title(f"Bar chart of {column} (Top {max_categories})")
        else:
            plt.title(f"Bar chart of {column}")
        sns.barplot(x=value_counts.index.astype(str), y=value_counts.values, palette="viridis")
        plt.xlabel(column)
        plt.ylabel("ØªØ¹Ø¯Ø§Ø¯")
        plt.xticks(rotation=35, ha="right")
    plt.tight_layout()
    plt.show()


def log_transform(df: pd.DataFrame, column: str, new_column_name: str, constant: float = 1) -> pd.DataFrame:
    """
    ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ Ø¨Ø± Ø±ÙˆÛŒ ÛŒÚ© Ø³ØªÙˆÙ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ
    - Ù„Ú¯Ø§Ø±ÛŒØªÙ… Ø·Ø¨ÛŒØ¹ÛŒ (ln) Ø±ÙˆÛŒ (column + constant)
    - Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ constant=1 Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² log(0)
    - Ø§Ú¯Ø± Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†ÙÛŒ ÛŒØ§ ØµÙØ± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ØŒ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ constant Ø¨Ø²Ø±Ú¯ØªØ± ØªÙ†Ø¸ÛŒÙ… Ø´ÙˆØ¯ ØªØ§ ØªÙ…Ø§Ù… Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø«Ø¨Øª Ø´ÙˆÙ†Ø¯

    Args:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        column (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ù‡Ø¯Ù
        new_column_name (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯
        constant (float): Ø¹Ø¯Ø¯ Ø«Ø§Ø¨ØªÛŒ Ú©Ù‡ Ø¨Ù‡ Ø³ØªÙˆÙ† Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶ 1)
        
    Returns:
        pd.DataFrame: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯
    """
    print("\n---ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ---")
    if column not in df.columns:
        raise ValueError(f"Ø³ØªÙˆÙ† '{column}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")

    transformed = np.log(df[column] + constant)
    df[new_column_name] = transformed
    print(f"â­ï¸ Ø³ØªÙˆÙ† Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ '{new_column_name}' Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙØ²ÙˆØ¯Ù‡ Ø´Ø¯.")
    return df

# ---------------------------------------------------------------------------------------------------
# 5) label_encode
# ---------------------------------------------------------------------------------------------------
def label_encode_column(df: pd.DataFrame, column: str) -> pd.DataFrame:
    """
    Ø¹Ù…Ù„ Label Encoding Ø±Ø§ Ø±ÙˆÛŒ ÛŒÚ© Ø³ØªÙˆÙ† Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯Ù‡ Ùˆ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù‡Ù…Ø§Ù† Ø³ØªÙˆÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ (NaN) Ø¨Ù‡ -1 Ù†Ú¯Ø§Ø´Øª Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù†Ú¯Ø§Ø´Øª Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        column (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±

    Ø®Ø±ÙˆØ¬ÛŒ:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ† encode Ø´Ø¯Ù‡ (Ø¯Ø± Ù‡Ù…Ø§Ù† Ø³ØªÙˆÙ†)

    Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:
        label_encode_column(df_loans, "purpose")
    """

    print("\n---Ø¹Ù…Ù„ÛŒØ§Øª label_encode---")

    if column not in df.columns:
        print(f"âŒ Ø³ØªÙˆÙ† '{column}' Ø¯Ø± Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return df

    col_data = df[column].copy()
    encoded, uniques = pd.factorize(col_data)
    df[column] = encoded

    # Ù†Ù…Ø§ÛŒØ´ Ù†Ú¯Ø§Ø´Øª Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ø¯Ø¯
    print(f"ğŸ”¢ Label Encoding Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† '{column}':")
    for idx, val in enumerate(uniques):
        print(f"  '{val}' â†’ {idx}")

    if -1 in encoded:
        print("  âš ï¸ ØªÙˆØ¬Ù‡: Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ù‡ -1 ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯")

    return df

# ---------------------------------------------------------------------------------------------------
#  6) Ø§Ø®ØªÙ„Ø§Ù ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ² Ø¨ÛŒÙ† Ø¯Ùˆ Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ®
# ---------------------------------------------------------------------------------------------------
def add_days_between_columns(df: pd.DataFrame, start_col: str, end_col: str, new_col_name: str) -> pd.DataFrame:
    """
    Ø§Ø®ØªÙ„Ø§Ù ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ² Ø¨ÛŒÙ† Ø¯Ùˆ Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        start_col (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ø´Ø±ÙˆØ¹ (Ù…Ø«Ù„Ø§Ù‹ 'loan_start')
        end_col (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† ØªØ§Ø±ÛŒØ® Ù¾Ø§ÛŒØ§Ù† (Ù…Ø«Ù„Ø§Ù‹ 'loan_end')
        new_col_name (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ Ø®Ø±ÙˆØ¬ÛŒ (Ù…Ø«Ù„Ø§Ù‹ 'days_diff')

    Ø¨Ø§Ø²Ú¯Ø´Øª:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯

    Ù…Ø«Ø§Ù„:
        add_days_between_columns(df_loans, "loan_start", "loan_end", "duration_days")
    """
    print("\n---Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø®ØªÙ„Ø§Ù Ø¨ÛŒÙ† Ø¯Ùˆ ØªØ§Ø±ÛŒØ®---")

    if start_col not in df.columns or end_col not in df.columns:
        print(f"âŒ ÛŒÚ©ÛŒ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ '{start_col}' ÛŒØ§ '{end_col}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return df

    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªØ§Ø±ÛŒØ®
    df[start_col] = pd.to_datetime(df[start_col], errors='coerce')
    df[end_col] = pd.to_datetime(df[end_col], errors='coerce')

    # Ø§Ø®ØªÙ„Ø§Ù Ø±ÙˆØ²Ù‡Ø§
    df[new_col_name] = (df[end_col] - df[start_col]).dt.days

    print(f"ğŸŸ¢ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ '{new_col_name}' (ØªØ¹Ø¯Ø§Ø¯ Ø±ÙˆØ² Ø¨ÛŒÙ† '{start_col}' Ùˆ '{end_col}') Ø¨Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø§ÙØ²ÙˆØ¯Ù‡ Ø´Ø¯.")
    return df

# ---------------------------------------------------------------------------------------------------
#  7) Ø¶Ø±Ø¨ Ø¯Ùˆ Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ
# ---------------------------------------------------------------------------------------------------
def multiply_columns(df: pd.DataFrame, col1: str, col2: str, new_col_name: str) -> pd.DataFrame:
    """
    Ø¶Ø±Ø¨ Ø¯Ùˆ Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ Ùˆ Ø¯Ø±Ø¬ Ù†ØªÛŒØ¬Ù‡ Ø¯Ø± ÛŒÚ© Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯.

    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        col1 (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø§ÙˆÙ„
        col2 (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø¯ÙˆÙ…
        new_col_name (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯

    Ø¨Ø§Ø²Ú¯Ø´Øª:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯

    Ù…Ø«Ø§Ù„:
        multiply_columns(df_loans, "rate", "loan_amount", "total_payment")
    """
    print("\n---Ø¶Ø±Ø¨ Ø¯Ùˆ Ø³ØªÙˆÙ†---")

    if col1 not in df.columns or col2 not in df.columns:
        print(f"âŒ ÛŒÚ©ÛŒ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ '{col1}' ÛŒØ§ '{col2}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return df

    if not pd.api.types.is_numeric_dtype(df[col1]) or not pd.api.types.is_numeric_dtype(df[col2]):
        print("â—ï¸ Ù‡Ø± Ø¯Ùˆ Ø³ØªÙˆÙ† Ø¨Ø§ÛŒØ¯ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø§Ø´Ù†Ø¯.")
        return df

    df[new_col_name] = df[col1] * df[col2]
    print(f"ğŸŸ¢ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ '{new_col_name}' (Ø¶Ø±Ø¨ '{col1}' Ø¯Ø± '{col2}') Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯.")
    return df

# ---------------------------------------------------------------------------------------------------
#  8) ØªÙ‚Ø³ÛŒÙ… Ø¯Ùˆ Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ
# ---------------------------------------------------------------------------------------------------
def divide_columns(df: pd.DataFrame, numerator_col: str, denominator_col: str, new_col_name: str) -> pd.DataFrame:
    """
    ÛŒÚ© Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ Ù†Ø³Ø¨Øª (ØªÙ‚Ø³ÛŒÙ…) Ø¯Ùˆ Ø³ØªÙˆÙ† Ø¹Ø¯Ø¯ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
    Ø§Ú¯Ø± Ù…Ø®Ø±Ø¬ ØµÙØ± ÛŒØ§ NaN Ø¨Ø§Ø´Ø¯ Ù†ØªÛŒØ¬Ù‡ NaN Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯ Ùˆ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ù‡Ø´Ø¯Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        numerator_col (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† ØµÙˆØ±Øª
        denominator_col (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ù…Ø®Ø±Ø¬
        new_col_name (str): Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯

    Ø¨Ø§Ø²Ú¯Ø´Øª:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯

    Ù…Ø«Ø§Ù„:
        divide_columns(df_loans, "rate_times_loan_amount", "duration_days", "rate_amount_per_day")
    """
    import pandas as pd
    import numpy as np

    if numerator_col not in df.columns or denominator_col not in df.columns:
        print(f"âŒ ÛŒÚ©ÛŒ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ '{numerator_col}' ÛŒØ§ '{denominator_col}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return df

    bad_rows = df[denominator_col].isna() | (df[denominator_col] == 0)
    n_bad = bad_rows.sum()
    if n_bad > 0:
        print(f"âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: {n_bad} Ø³Ø·Ø± Ø¯Ø§Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø± ØµÙØ± ÛŒØ§ NaN Ø¯Ø± '{denominator_col}' Ù‡Ø³ØªÙ†Ø¯Ø› Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø®Ø´â€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¢Ù†Ù‡Ø§ NaN Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

    df[new_col_name] = np.where(
        bad_rows,
        np.nan,
        df[numerator_col] / df[denominator_col]
    )
    print(f"ğŸŸ¢ Ø³ØªÙˆÙ† Ø¬Ø¯ÛŒØ¯ '{new_col_name}' = '{numerator_col}' ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± '{denominator_col}' Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
    return df

# ---------------------------------------------------------------------------------------------------  
#  9) Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§  
# ---------------------------------------------------------------------------------------------------  
def standardize_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:
    """
    Ù…Ù‚Ø§Ø¯ÛŒØ± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        columns (list): Ù„ÛŒØ³Øª Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø´ÙˆÙ†Ø¯
    
    Ø¨Ø§Ø²Ú¯Ø´Øª:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
    """
    print("\n---Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§  ---")
    for col in columns:
        if col in df.columns:
            mean = df[col].mean()
            std_dev = df[col].std()
            df[col] = (df[col] - mean) / std_dev
            print(f"ğŸŸ¢ Ø³ØªÙˆÙ† '{col}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø´Ø¯.")
        else:
            print(f"âŒ Ø³ØªÙˆÙ† '{col}' Ø¯Ø± Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    return df

# ---------------------------------------------------------------------------------------------------
#  10) Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
# ---------------------------------------------------------------------------------------------------
def drop_columns(df: pd.DataFrame, columns: list) -> pd.DataFrame:
    """
    Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ø² Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        columns (list): Ù„ÛŒØ³Øª Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
    
    Ø¨Ø§Ø²Ú¯Ø´Øª:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡
    """
    print("\n--- Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§  ---")
    for col in columns:
        if col in df.columns:
            df = df.drop(col, axis=1)
            print(f"ğŸŸ¢ Ø³ØªÙˆÙ† '{col}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø­Ø°Ù Ø´Ø¯.")
        else:
            print(f"âŒ Ø³ØªÙˆÙ† '{col}' Ø¯Ø± Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    return df


# ---------------------------------------------------------------------------------------------------
#  10) Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ù‡ CSV
# ---------------------------------------------------------------------------------------------------
def save_dataframe(df, output_dir="data", filename="processed_data", index=False):
    """
    Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª CSV Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ù…Ø´Ø®Øµâ€ŒØ´Ø¯Ù‡ Ùˆ Ø¨Ø§ Ù†Ø§Ù… Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df (pd.DataFrame): Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù…ÙˆØ±Ø¯Ù†Ø¸Ø±
        output_dir (str): Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ (Ù¾ÛŒØ´ ÙØ±Ø¶ "data")
        filename (str): Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø¯ÙˆÙ† Ù¾Ø³ÙˆÙ†Ø¯ (Ù¾ÛŒØ´ ÙØ±Ø¶ "processed_data")
        index (bool): Ø°Ø®ÛŒØ±Ù‡ Ø§Ù†Ø¯ÛŒØ³ Ø³Ø·Ø±Ù‡Ø§ ÛŒØ§ Ø®ÛŒØ± (Ù¾ÛŒØ´ ÙØ±Ø¶ False)
    """
    print("\n---Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…---")
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    output_path = os.path.join(output_dir, f"{filename}.csv")
    df.to_csv(output_path, index=index, encoding="utf-8-sig")
    print(f"ğŸ’¾ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± '{output_path}' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")

# ---------------------------------------------------------------------------------------------------
#  11) ØªÙ‚Ø³ÛŒÙ… Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ†
# ---------------------------------------------------------------------------------------------------
def split_and_save_train_test(df, test_size=0.2, random_state=42, 
                              output_dir="data", train_name="train_set", test_name="test_set", index=False):
    """
    Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ† ØªÙ‚Ø³ÛŒÙ… Ú©Ø±Ø¯Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    
    Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
        df : Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        test_size : Ù†Ø³Ø¨Øª Ø¢Ø²Ù…ÙˆÙ† (Ø¨ÛŒÙ† Û° Ùˆ Û±)
        random_state : Ø¹Ø¯Ø¯ Ø«Ø§Ø¨Øª Ø¨Ø±Ø§ÛŒ ØªÚ©Ø±Ø§Ø±Ù¾Ø°ÛŒØ±ÛŒ
        output_dir : Ù¾ÙˆØ´Ù‡ Ø°Ø®ÛŒØ±Ù‡  
        train_name : Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¨Ø®Ø´ Ø¢Ù…ÙˆØ²Ø´  
        test_name : Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¨Ø®Ø´ Ø¢Ø²Ù…ÙˆÙ†  
        index : Ø¢ÛŒØ§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø°Ø®ÛŒØ±Ù‡ Ø´ÙˆØ¯ ÛŒØ§ Ù†Ù‡
    
    Ø¨Ø§Ø²Ú¯Ø´Øª:
        df_train, df_test
    """
    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state)
    
    save_dataframe(df_train, output_dir=output_dir, filename=train_name, index=index)
    save_dataframe(df_test, output_dir=output_dir, filename=test_name, index=index)
    print(f"âœ… ØªÙ‚Ø³ÛŒÙ… Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ† Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    return df_train, df_test

# ---------------------------------------------------------------------------------------------------
#  Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
# ---------------------------------------------------------------------------------------------------

csv_path = "./data/loans.csv"
# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª
df_loans = load_and_inspect_data(csv_path)

# 2. ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
# Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯
columns_for_outlier_analysis = ["loan_amount", "rate"] 

# 2.1. ÙÙ‚Ø· ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
analyze_outliers(df_loans, columns_for_outlier_analysis)

# 2.2. ÙÙ‚Ø· Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
visualize_outliers(df_loans, columns_for_outlier_analysis, save_base_name="outlier_analysis")

# 3. ØªØ¨Ø¯ÛŒÙ„ Ù„Ú¯Ø§Ø±ÛŒØªÙ…ÛŒ Ø¨Ø± Ø±ÙˆÛŒ ÛŒÚ© Ø³ØªÙˆÙ†
# visualize_distribution(df_loans, "rate")
df_loans = log_transform(df_loans, column="rate", new_column_name="log_rate")

# 4. Label Encoding 
column_to_encode = "loan_type"  
df_loans = label_encode_column(df_loans, column_to_encode)

# 5. Ø§ÛŒØ¬Ø§Ø¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
df_loans = add_days_between_columns(df_loans, "loan_start", "loan_end", "duration_days")
df_loans = multiply_columns(df_loans, "rate", "loan_amount", "rate_times_loan_amount")
df_loans = divide_columns(df_loans, "rate_times_loan_amount", "duration_days", "amount_per_day")

# 6. Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
# Ù„ÛŒØ³Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø´ÙˆÙ†Ø¯
columns_to_standardize = ["loan_amount", "log_rate", "duration_days", "rate_times_loan_amount", "amount_per_day"]
df_loans = standardize_columns(df_loans, columns_to_standardize)

# 7. Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
# Ù„ÛŒØ³Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù Ø´ÙˆÙ†Ø¯
columns_to_drop = ["rate", "loan_id"]
df_loans = drop_columns(df_loans, columns_to_drop)



# 8. ØªÙ‚Ø³ÛŒÙ… Ùˆ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…ÙˆÙ†
df_train, df_test = split_and_save_train_test(df_loans, test_size=0.2, random_state=42)


# 9. Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù†Ù‡Ø§ÛŒÛŒ
save_dataframe(df_loans, output_dir="data", filename="loans_final", index=False)